{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Modeling: LSTM for Stock Price Movement Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add src directory to Python path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.data_loader import download_stock_data\n",
    "from src.feature_engineering import add_technical_indicators, add_rolling_lag_features, create_target_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER = 'AAPL'\n",
    "START_DATE = '2018-01-01' # Needs enough data for sequence creation and TIs\n",
    "END_DATE = '2023-12-31'\n",
    "INTERVAL = '1d'\n",
    "\n",
    "FUTURE_DAYS_TARGET = 5\n",
    "PERCENT_CHANGE_THRESHOLD = 0.03\n",
    "\n",
    "ROLLING_WINDOWS = [5, 10, 20, 60]\n",
    "LAG_PERIODS = [1, 2, 3, 5, 10]\n",
    "KEY_LAG_INDICATORS = ['RSI_14', 'MACD', 'ATR_14', 'Stoch_k', 'ADX_14']\n",
    "\n",
    "SEQUENCE_LENGTH = 20 # Number of past days' data to use for predicting next target\n",
    "N_SPLITS_TIMESERIES = 5 # For TimeSeriesSplit\n",
    "\n",
    "# LSTM Model Parameters\n",
    "LSTM_UNITS = 50\n",
    "DROPOUT_RATE = 0.2\n",
    "EPOCHS = 50 # Increased epochs, with early stopping\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MODEL_SAVE_PATH = '../models/lstm_stock_predictor.h5' # Path to save the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Engineer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Download Data\n",
    "raw_data = download_stock_data([TICKER], START_DATE, END_DATE, INTERVAL)\n",
    "if raw_data is None:\n",
    "    raise ValueError(f\"Failed to download data for {TICKER}. Halting execution.\")\n",
    "print(f\"Raw data shape: {raw_data.shape}\")\n",
    "\n",
    "# 3.2 Add Technical Indicators\n",
    "data_with_ti = add_technical_indicators(raw_data.copy(), fillna=True)\n",
    "print(f\"Shape after TIs: {data_with_ti.shape}\")\n",
    "\n",
    "# 3.3 Add Rolling and Lag Features\n",
    "data_with_roll_lag = add_rolling_lag_features(\n",
    "    data_with_ti.copy(),\n",
    "    windows=ROLLING_WINDOWS,\n",
    "    lags=LAG_PERIODS,\n",
    "    lag_indicators=KEY_LAG_INDICATORS\n",
    ")\n",
    "print(f\"Shape after rolling/lag features: {data_with_roll_lag.shape}\")\n",
    "\n",
    "# 3.4 Create Target Variable\n",
    "processed_data = create_target_variable(\n",
    "    data_with_roll_lag.copy(), \n",
    "    future_days=FUTURE_DAYS_TARGET, \n",
    "    percent_change_threshold=PERCENT_CHANGE_THRESHOLD\n",
    ")\n",
    "print(f\"Shape after target creation: {processed_data.shape}\")\n",
    "\n",
    "# 3.5 Define Features (X) and Target (y) and Drop NaNs\n",
    "base_price_volume_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "feature_columns = [col for col in processed_data.columns if col not in base_price_volume_cols + ['target']]\n",
    "\n",
    "X_raw_full = processed_data[feature_columns]\n",
    "y_raw_full = processed_data['target']\n",
    "\n",
    "print(f\"Number of features: {len(X_raw_full.columns)}\")\n",
    "print(f\"Original shape before NaN drop: X: {X_raw_full.shape}, y: {y_raw_full.shape}\")\n",
    "\n",
    "combined_for_cleaning = X_raw_full.assign(target=y_raw_full)\n",
    "cleaned_data = combined_for_cleaning.dropna()\n",
    "\n",
    "X_cleaned_full = cleaned_data[feature_columns]\n",
    "y_cleaned_full = cleaned_data['target']\n",
    "\n",
    "print(f\"Shape after NaN drop: X_cleaned: {X_cleaned_full.shape}, y_cleaned: {y_cleaned_full.shape}\")\n",
    "\n",
    "if X_cleaned_full.empty or y_cleaned_full.empty:\n",
    "    raise ValueError(\"No data left after NaN removal. Adjust parameters or data range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Split Data using TimeSeriesSplit\n",
    "We use TimeSeriesSplit to get train/test indices. For this example, we'll use the last split as our primary train/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=N_SPLITS_TIMESERIES)\n",
    "train_index, test_index = None, None # Initialize\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X_cleaned_full)):\n",
    "    print(f\"Split {i+1}: Train indices: {len(train_idx)}, Test indices: {len(test_idx)}\")\n",
    "    if i == N_SPLITS_TIMESERIES - 1: # Use the last split for train/test\n",
    "        train_index, test_index = train_idx, test_idx\n",
    "\n",
    "X_train_raw, X_test_raw = X_cleaned_full.iloc[train_index], X_cleaned_full.iloc[test_index]\n",
    "y_train_raw, y_test_raw = y_cleaned_full.iloc[train_index], y_cleaned_full.iloc[test_index]\n",
    "\n",
    "print(f\"\\nUsing last split: X_train_raw shape: {X_train_raw.shape}, X_test_raw shape: {X_test_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Scale Features\n",
    "Fit scaler ONLY on training data, then transform both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "print(\"Data scaled. X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X_data, y_data, sequence_length):\n",
    "    X_sequences, y_sequences = [], []\n",
    "    for i in range(len(X_data) - sequence_length):\n",
    "        X_sequences.append(X_data[i:i + sequence_length])\n",
    "        y_sequences.append(y_data.iloc[i + sequence_length]) # Target is for the day after the sequence\n",
    "    return np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_raw, SEQUENCE_LENGTH)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_raw, SEQUENCE_LENGTH)\n",
    "\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}, y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"X_test_seq shape: {X_test_seq.shape}, y_test_seq shape: {y_test_seq.shape}\")\n",
    "\n",
    "if X_train_seq.shape[0] == 0 or X_test_seq.shape[0] == 0:\n",
    "    raise ValueError(\"Not enough data to create sequences after train/test split. Increase data range or decrease SEQUENCE_LENGTH.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define and Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(LSTM_UNITS, input_shape=(SEQUENCE_LENGTH, X_train_seq.shape[2]), return_sequences=False), # return_sequences=True if stacking LSTMs\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    Dense(LSTM_UNITS // 2, activation='relu'), # Optional intermediate dense layer\n",
    "    Dropout(DROPOUT_RATE / 2),\n",
    "    Dense(1, activation='sigmoid') # Output layer for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    y_train_seq,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.1, # Use a portion of training data for validation during training\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "if 'auc' in history.history and 'val_auc' in history.history:\n",
    "    plt.plot(history.history['auc'], label='Train AUC')\n",
    "    plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "plt.title('Accuracy/AUC Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_lstm = model.predict(X_test_seq)\n",
    "y_pred_lstm = (y_pred_proba_lstm > 0.5).astype(int) # Apply threshold\n",
    "\n",
    "print(\"\\nLSTM Model Evaluation:\")\n",
    "accuracy_lstm = accuracy_score(y_test_seq, y_pred_lstm)\n",
    "print(f\"Accuracy: {accuracy_lstm:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_seq, y_pred_lstm, zero_division=0))\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr_lstm, tpr_lstm, _ = roc_curve(y_test_seq, y_pred_proba_lstm)\n",
    "roc_auc_lstm = auc(fpr_lstm, tpr_lstm)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_lstm, tpr_lstm, color='darkorange', lw=2, label=f'LSTM ROC curve (area = {roc_auc_lstm:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('LSTM Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "print(f\"AUC Score: {roc_auc_lstm:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lstm = confusion_matrix(y_test_seq, y_pred_lstm)\n",
    "disp_lstm = ConfusionMatrixDisplay(confusion_matrix=cm_lstm)\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "disp_lstm.plot(ax=ax, cmap=plt.cm.Blues)\n",
    "ax.set_title('LSTM Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix values:\")\n",
    "print(f\"True Negatives (TN): {cm_lstm[0, 0]}\")\n",
    "print(f\"False Positives (FP): {cm_lstm[0, 1]}\")\n",
    "print(f\"False Negatives (FN): {cm_lstm[1, 0]}\")\n",
    "print(f\"True Positives (TP): {cm_lstm[1, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.dirname(MODEL_SAVE_PATH)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    print(f\"Created directory: {model_dir}\")\n",
    "\n",
    "model.save(MODEL_SAVE_PATH)\n",
    "print(f\"\\nLSTM model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (Placeholder) Optuna Hyperparameter Tuning\n",
    "This section would contain the setup for Optuna to find optimal hyperparameters for the LSTM model. Due to execution time constraints in this environment, the actual Optuna study is not run here but the structure would be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def create_lstm_model_optuna(trial, X_train_shape):\n",
    "    # Hyperparameters to tune\n",
    "    lstm_units = trial.suggest_int('lstm_units', 32, 128, step=16)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
    "    dense_units = trial.suggest_int('dense_units', 16, 64, step=16)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, input_shape=(X_train_shape[1], X_train_shape[2]), return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate / 2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "def objective_lstm(trial, X_train_s, y_train_s, X_val_s, y_val_s):\n",
    "    model = create_lstm_model_optuna(trial, X_train_s.shape)\n",
    "    \n",
    "    # Train the model (using a subset or fewer epochs for faster Optuna trials)\n",
    "    history = model.fit(\n",
    "        X_train_s, y_train_s,\n",
    "        epochs=10, # Reduced epochs for Optuna trials\n",
    "        batch_size=BATCH_SIZE, \n",
    "        validation_data=(X_val_s, y_val_s),\n",
    "        callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
    "        verbose=0 # Suppress verbose output during Optuna trials\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    val_auc = history.history['val_auc'][-1] # Using AUC as the metric to optimize\n",
    "    return val_auc # Optuna tries to maximize this\n",
    "\n",
    "print(\"\\nOptuna setup for LSTM hyperparameter tuning (not run in this script due to time constraints).\")\n",
    "print(\"To run Optuna, you would typically do something like:\")\n",
    "print(\"# 1. Prepare validation data (X_val_seq, y_val_seq) from a split of the training set.\")\n",
    "print(\"# study = optuna.create_study(direction='maximize')\")\n",
    "print(\"# study.optimize(lambda trial: objective_lstm(trial, X_train_seq, y_train_seq, X_val_seq, y_val_seq), n_trials=10) # e.g., 10 trials\")\n",
    "print(\"# print('Best trial:', study.best_trial.params)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
